{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS__mzEiZ6Ew",
        "outputId": "944a10f5-b707-465b-f2b9-2382ac4b95c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xi9icUpOy4oG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import math\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import yaml\n",
        "import random\n",
        "from google.colab import files\n",
        "import sys\n",
        "import time\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 檢查是否有可用的 CUDA 設備（通常是顯卡，支援 GPU 運算），如果有，就將 device 變數設置為 \"cuda\"，否則設置為 \"cpu\"。\n",
        "from __future__ import print_function\n",
        "import zipfile\n",
        "import os\n",
        "import pdb\n",
        "import torch\n",
        "import h5py\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms, utils\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "from torch.autograd import Variable\n",
        "# from logger import Logger\n",
        "import pdb\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_config(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config"
      ],
      "metadata": {
        "id": "adC1eOcqZuaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model/config/config.yml' # main\n",
        "# file_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Model/config/config.yml' # free\n",
        "config = load_config(file_path)"
      ],
      "metadata": {
        "id": "IdtTcF29ZvCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_height = 64\n",
        "output_width = 64\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "ZjgUAeX6wx0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransposeDepthInput(object):\n",
        "    def __call__(self, depth):\n",
        "        depth = depth.transpose((2, 0, 1))\n",
        "        depth = torch.from_numpy(depth)\n",
        "        depth = depth.view(1, depth.shape[0], depth.shape[1], depth.shape[2])\n",
        "        depth = nn.functional.interpolate(depth, size=(output_height, output_width), mode='bilinear', align_corners=False)\n",
        "        # depth = torch.log(depth)\n",
        "        return depth[0]"
      ],
      "metadata": {
        "id": "VId-4IUyzb-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rgb_data_transforms = transforms.Compose([\n",
        "    transforms.Resize((output_height, output_width)),    # Different for Input Image & Depth Image\n",
        "    transforms.ToTensor(),\n",
        "    ])"
      ],
      "metadata": {
        "id": "SaP1szmnzxHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depth_data_transforms = transforms.Compose([\n",
        "    TransposeDepthInput(),\n",
        "])"
      ],
      "metadata": {
        "id": "bL0NTcvpz1Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_for_plot_transforms = transforms.Compose([\n",
        "    transforms.Resize((output_height, output_width)),    # Different for Input Image & Depth Image\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "ozRr-fLGz3U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NYUDataset(Dataset):\n",
        "    def calculate_mean(self, images):\n",
        "        mean_image = np.mean(images, axis=0)\n",
        "        return mean_image\n",
        "\n",
        "    def __init__(self, filename, type, rgb_transform = None, depth_transform = None):\n",
        "        f = h5py.File(filename, 'r')\n",
        "        if type == \"training\":\n",
        "            self.images = f['images'][0:1400]\n",
        "            self.depths = f['depths'][0:1400]\n",
        "        elif type == \"validation\":\n",
        "            self.images = f['images'][1024:1248]\n",
        "            self.depths = f['depths'][1024:1248]\n",
        "        elif type == \"test\":\n",
        "            self.images = f['images'][1400:]\n",
        "            self.depths = f['depths'][1400:]\n",
        "        self.rgb_transform = rgb_transform\n",
        "        self.depth_transform = depth_transform\n",
        "        self.mean_image = self.calculate_mean(f['images'][0:1449])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        # image = (image - self.mean_image)/np.std(image)\n",
        "        image = image.transpose((2, 1, 0))\n",
        "        # image = (image - image.min())/(image.max() - image.min())\n",
        "        # image = image * 255\n",
        "        # image = image.astype('uint8')\n",
        "        image = Image.fromarray(image)\n",
        "        if self.rgb_transform:\n",
        "            image = self.rgb_transform(image)\n",
        "        depth = self.depths[idx]\n",
        "        depth = np.reshape(depth, (1, depth.shape[0], depth.shape[1]))\n",
        "        depth = depth.transpose((2, 1, 0))\n",
        "        if self.depth_transform:\n",
        "            depth = self.depth_transform(depth)\n",
        "        sample = {'image': image, 'depth': depth}\n",
        "        return sample"
      ],
      "metadata": {
        "id": "n66TbtLz0Gak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/nyu_depth_v2_labeled.mat' # for free account\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/nyu_depth_v2_labeled.mat' # for main account\n",
        "train_loader = torch.utils.data.DataLoader(NYUDataset( path,\n",
        "                                                       'training',\n",
        "                                                        rgb_transform = rgb_data_transforms,\n",
        "                                                        depth_transform = depth_data_transforms),\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = True, num_workers = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAQ6uF7F0uni",
        "outputId": "a1abadd4-0147-45be-be99-d7e5cda51b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(NYUDataset( path,\n",
        "                                                       'test',\n",
        "                                                        rgb_transform = rgb_data_transforms,\n",
        "                                                        depth_transform = depth_data_transforms),\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False, num_workers = 5)\n",
        "\n",
        "\n",
        "# train_loader = val_loader"
      ],
      "metadata": {
        "id": "gWDCKlJLzRUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Upsample(nn.Module): # this\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,  # this conv let the size unchanged\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(\n",
        "            x, scale_factor=2.0, mode=\"nearest\") # double the size\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JkWjVUn_WSXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=2,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0, 1, 0, 1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nIng2nc5WXJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.Lrelu = nn.ELU()\n",
        "        # self.Lrelu = nonlinearity\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 =nn.BatchNorm2d(in_channels)     # 這裡上面define的Normalize有點像是class的感覺\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, # size unchanged\n",
        "                                     out_channels,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)\n",
        "\n",
        "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout) # param為機率\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, # size unchanged\n",
        "                                     out_channels,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels,   # size unchanged\n",
        "                                                     out_channels,\n",
        "                                                     kernel_size=3,\n",
        "                                                     stride=1,\n",
        "                                                     padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels,    # size unchanged\n",
        "                                                    out_channels,\n",
        "                                                    kernel_size=1,\n",
        "                                                    stride=1,\n",
        "                                                    padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        h = self.norm1(h)    # normalize\n",
        "\n",
        "        h = self.Lrelu(h)  # sigmoid\n",
        "        h = self.conv1(h)    # channel become out_channel\n",
        "\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = self.Lrelu(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:  # 如果inchannel和outchannel不同需要把輸入值channel也調整成一樣，用上conv2D，若inchannel和outchannel一樣就直接加\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h"
      ],
      "metadata": {
        "id": "PFPVGgP4Wa51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modified\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = nn.BatchNorm2d(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels, # in_cha == out_cha and the kernel size = 1, and size unchanged\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "        self.norm2 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b, c, h, w = q.shape # (batch, channel, height, width)\n",
        "        q = q.reshape(b, c, h*w)\n",
        "        q = q.permute(0, 2, 1)   # b,hw,c.  # 此兩變換(reshape + permute)詳細情形如下格，簡單來說最外圈還是每一個data(一張照片)，\n",
        "                                # 往內一圈則是把該資料的所有channel(rgb)的同個位置放在一起\n",
        "        k = k.reshape(b, c, h*w)  # b,c,hw # 單一此變換則是外圈是data，向內一圈則是該data一個channel內所有的值\n",
        "        w_ = torch.bmm(q, k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        # 注意這邊是q * k，是把不同channels的同個位置變成vector然後內積，這樣跟cnn最大的差別是cnn只會在同一個區塊做相關性，attention卻在channels中的每個位置會相互做相關性\n",
        "        w_ = w_ * (int(c)**(-0.5)) # 為何不是 * (int(h * w) ** (-0.5))?\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b, c, h*w)\n",
        "        w_ = w_.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n",
        "        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = torch.bmm(v, w_)\n",
        "        h_ = h_.reshape(b, c, h, w)\n",
        "\n",
        "        h_ = self.norm2(self.proj_out(h_))\n",
        "\n",
        "\n",
        "        return x+h_"
      ],
      "metadata": {
        "id": "KJVHxiE5Wdrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ELU()\n",
        "        # self.Lrelu = nn.ELU()\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nkMJSji_pPtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, config, want_print):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.print = want_print\n",
        "        ch, out_ch, ch_mult = config['model']['ch'], config['model']['out_ch'], tuple(config['model']['ch_mult'])\n",
        "        # ch = 128, out_ch = 3\n",
        "        num_res_blocks = config['model']['num_res_blocks']\n",
        "        # num_res_blocks = 2\n",
        "        attn_resolutions = config['model']['attn_resolutions']\n",
        "        attn_resolutions = [32, ]\n",
        "        # attn_resolution = [16, ]\n",
        "        dropout = config['model']['dropout']\n",
        "        # dropout = 0.0\n",
        "        in_channels = config['model']['in_channels']\n",
        "        # in_channels = 3\n",
        "        resolution = config['data']['image_size']\n",
        "        resolution = 64\n",
        "        # resolution = 256\n",
        "        resamp_with_conv = config['model']['resamp_with_conv']\n",
        "        # resamp_with_conv = True\n",
        "\n",
        "\n",
        "\n",
        "        if config['model']['type'] == 'bayesian':\n",
        "            self.logvar = nn.Parameter(torch.zeros(num_timesteps))\n",
        "\n",
        "        self.ch = ch\n",
        "        ch_mult = (1, 2, 2)\n",
        "        # ch_mult = (1, 2)\n",
        "\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        # num_resolutions = 4\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# nonlinear\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+ch_mult\n",
        "        # in_ch_mult = (1, 1, 1, 2, 2)\n",
        "\n",
        "        self.conv_down1 = ConvBlock(3, 16)\n",
        "        self.conv_down2 = ConvBlock(16, 32)\n",
        "        self.conv_down3 = ConvBlock(32, 64)\n",
        "        self.conv_down4 = ConvBlock(64, 128)\n",
        "\n",
        "        self.down = nn.ModuleList()\n",
        "        block_in = None\n",
        "\n",
        "\n",
        "        # self.conv_in = torch.nn.Conv2d(3,\n",
        "        #                                self.ch,\n",
        "        #                                kernel_size=3,\n",
        "        #                                stride=1,\n",
        "        #                                padding=1)\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    print(curr_res)\n",
        "                    attn.append(AttnBlock(block_in))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = AttnBlock(block_in)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level] # 最後一個block時inchannel數可能變少\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in,\n",
        "                                         out_channels=block_out,\n",
        "\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(AttnBlock(block_in))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up)  # prepend to get consistent order, (0, up)代表在ModuleList()的0位置插入up這個Module\n",
        "\n",
        "        # end\n",
        "        self.norm_out = nn.BatchNorm2d(block_in)\n",
        "\n",
        "        self.Lrelu = nn.ELU()\n",
        "\n",
        "        self.conv_up1 = ConvBlock(128, 64)\n",
        "        self.conv_up2 = ConvBlock(64, 32)\n",
        "        self.conv_up3 = ConvBlock(32, 16)\n",
        "        self.conv_up4 = ConvBlock(16, 3)\n",
        "        self.conv_up5 = ConvBlock(3, 1)\n",
        "    def forward(self, image):\n",
        "\n",
        "\n",
        "\n",
        "        # downsampling down 裡面有好幾個元素，每個元素包含block(resblock), attn, downsample，其中attn只有幾個元素會有，downsample除了最後一個元素以外都有\n",
        "        # 整個流程就是把x送進conv2D 然後送進down裡面經過resblock和部份attn downsample(conv2D)\n",
        "        # hs = [self.conv_in(image)]\n",
        "        h = image\n",
        "        if self.print:\n",
        "            print(\"original {}\".format(h.shape))\n",
        "        h = self.conv_down1(h)\n",
        "        if self.print:\n",
        "            print(\"conv_down1 {}\".format(h.shape))\n",
        "        h = self.conv_down2(h)\n",
        "        if self.print:\n",
        "            print(\"conv_down2 {}\".format(h.shape))\n",
        "        h = self.conv_down3(h)\n",
        "        if self.print:\n",
        "            print(\"conv_down3 {}\".format(h.shape))\n",
        "        h = self.conv_down4(h)\n",
        "        if self.print:\n",
        "            print(\"conv_down4 {}\".format(h.shape))\n",
        "        hs = [h]\n",
        "        if self.print:\n",
        "            print(hs[-1].shape)\n",
        "\n",
        "\n",
        "\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "\n",
        "                h = self.down[i_level].block[i_block](hs[-1]) # h 如果可以是attn就是attn 不然就是res, note that h是把值喂進去模塊後的值，要把hs的最後跟time embedded送進去\n",
        "                if self.print:\n",
        "                    print(\"res {}\".format(h.shape))\n",
        "\n",
        "\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                    if self.print:\n",
        "                        print(\"this is attn {}\".format(h.shape))\n",
        "\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                h_temp = self.down[i_level].downsample(hs[-1])\n",
        "                if self.print:\n",
        "                    print(\"downsample {}\".format(h_temp.shape))\n",
        "                hs.append(h_temp)\n",
        "        # return hs\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "\n",
        "        h = self.mid.block_1(h)\n",
        "        if self.print:\n",
        "            print(\"mid res {}\".format(h.shape))\n",
        "        h = self.mid.attn_1(h)\n",
        "        if self.print:\n",
        "            print(\"mid attn {}\".format(h.shape))\n",
        "        h = self.mid.block_2(h)\n",
        "        if self.print:\n",
        "            print(\"mid res {}\".format(h.shape))\n",
        "\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "\n",
        "                h_cat = torch.cat([h, hs.pop()], dim=1)\n",
        "                if self.print:\n",
        "                    print(\"this is cat {}\".format(h_cat.shape))\n",
        "                h = self.up[i_level].block[i_block](h_cat) # u-net的cat down\n",
        "                if self.print:\n",
        "                    print(\"res {}\".format(h.shape))\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "                    if self.print:\n",
        "                        print(\"this is attn {}\".format(h.shape))\n",
        "\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "                if self.print:\n",
        "                    print(\"this is upsample {}\".format(h.shape))\n",
        "\n",
        "\n",
        "        # end\n",
        "\n",
        "\n",
        "\n",
        "        h = self.norm_out(h)\n",
        "\n",
        "\n",
        "\n",
        "        h = self.Lrelu(h)\n",
        "\n",
        "        h = self.conv_up1(h)\n",
        "        if self.print:\n",
        "            print(\"conv_up1 {}\".format(h.shape))\n",
        "        h = self.conv_up2(h)\n",
        "        if self.print:\n",
        "            print(\"conv_up2 {}\".format(h.shape))\n",
        "        h = self.conv_up3(h)\n",
        "        if self.print:\n",
        "            print(\"conv_up3 {}\".format(h.shape))\n",
        "        h = self.conv_up4(h)\n",
        "        if self.print:\n",
        "            print(\"conv_up4 {}\".format(h.shape))\n",
        "        h = self.conv_up5(h)\n",
        "        if self.print:\n",
        "            print(\"conv_up5 {}\".format(h.shape))\n",
        "\n",
        "        return h\n"
      ],
      "metadata": {
        "id": "_6E0y2LCWlPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(config, False)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmpMuy39-szX",
        "outputId": "8a338b7f-22b8-427b-e98a-e49c179df0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (conv_down1): ConvBlock(\n",
              "    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0)\n",
              "  )\n",
              "  (conv_down2): ConvBlock(\n",
              "    (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0)\n",
              "  )\n",
              "  (conv_down3): ConvBlock(\n",
              "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0)\n",
              "  )\n",
              "  (conv_down4): ConvBlock(\n",
              "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0)\n",
              "  )\n",
              "  (down): ModuleList(\n",
              "    (0): Module(\n",
              "      (block): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (Lrelu): ELU(alpha=1.0)\n",
              "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "      (downsample): Downsample(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
              "      )\n",
              "    )\n",
              "    (1): Module(\n",
              "      (block): ModuleList(\n",
              "        (0): ResnetBlock(\n",
              "          (Lrelu): ELU(alpha=1.0)\n",
              "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ResnetBlock(\n",
              "          (Lrelu): ELU(alpha=1.0)\n",
              "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList(\n",
              "        (0-1): 2 x AttnBlock(\n",
              "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (downsample): Downsample(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
              "      )\n",
              "    )\n",
              "    (2): Module(\n",
              "      (block): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (Lrelu): ELU(alpha=1.0)\n",
              "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "    )\n",
              "  )\n",
              "  (mid): Module(\n",
              "    (block_1): ResnetBlock(\n",
              "      (Lrelu): ELU(alpha=1.0)\n",
              "      (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (attn_1): AttnBlock(\n",
              "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (block_2): ResnetBlock(\n",
              "      (Lrelu): ELU(alpha=1.0)\n",
              "      (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (up): ModuleList(\n",
              "    (0): Module(\n",
              "      (block): ModuleList(\n",
              "        (0): ResnetBlock(\n",
              "          (Lrelu): ELU(alpha=1.0)\n",
              "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1-2): 2 x ResnetBlock(\n",
              "          (Lrelu): ELU(alpha=1.0)\n",
              "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "    )\n",
              "    (1): Module(\n",
              "      (block): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (Lrelu): ELU(alpha=1.0)\n",
              "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): ResnetBlock(\n",
              "          (Lrelu): ELU(alpha=1.0)\n",
              "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList(\n",
              "        (0-2): 3 x AttnBlock(\n",
              "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (upsample): Upsample(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (2): Module(\n",
              "      (block): ModuleList(\n",
              "        (0-2): 3 x ResnetBlock(\n",
              "          (Lrelu): ELU(alpha=1.0)\n",
              "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attn): ModuleList()\n",
              "      (upsample): Upsample(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm_out): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (Lrelu): ELU(alpha=1.0)\n",
              "  (conv_up1): ConvBlock(\n",
              "    (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0)\n",
              "  )\n",
              "  (conv_up2): ConvBlock(\n",
              "    (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0)\n",
              "  )\n",
              "  (conv_up3): ConvBlock(\n",
              "    (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0)\n",
              "  )\n",
              "  (conv_up4): ConvBlock(\n",
              "    (conv): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0)\n",
              "  )\n",
              "  (conv_up5): ConvBlock(\n",
              "    (conv): Conv2d(3, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NO_epochs = 700\n",
        "save_frequency = 5\n",
        "LR = 0.001\n",
        "\n",
        "clip_value = 1e-2\n",
        "batch_size = 32\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "first_epoch = 0\n",
        "start = 460\n",
        "warm_up_period = 20"
      ],
      "metadata": {
        "id": "7ycY1hoAtoHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def warmup_scheduler(epoch):\n",
        "    if epoch < warm_up_period:\n",
        "        return (epoch / warm_up_period)\n",
        "    else:\n",
        "        return 1 / 100\n"
      ],
      "metadata": {
        "id": "O18G683utowe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = LambdaLR(optimizer, lr_lambda = warmup_scheduler)"
      ],
      "metadata": {
        "id": "NI7n6SAitqTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if first_epoch:\n",
        "    for epoch in range(1, NO_epochs + 1):\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print('lr in the last epoch: {}'.format(current_lr))\n",
        "        start_time = time.time()\n",
        "        mean_epoch_loss = []\n",
        "        mean_epoch_loss_val = []\n",
        "        epoch_gradient = {}\n",
        "        for image in train_loader:\n",
        "            scheduler.step(epoch)\n",
        "            input_img = image['image'].to(torch.float32).to(device)\n",
        "            target_depth = image['depth'].to(torch.float32).to(device)\n",
        "\n",
        "            pred_depth = model(input_img)\n",
        "            # --------------------exit--------\n",
        "            # sys.exit()\n",
        "            # --------------------exit--------\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n",
        "            # params = list(model.parameters())\n",
        "            # weight_tensor = params[490]\n",
        "            # chains = torch.autograd.grad(loss, weight_tensor, retain_graph=True)\n",
        "            # for chain in chains:\n",
        "            #     print(chain)\n",
        "            mean_epoch_loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            # for param in model.parameters():\n",
        "            #     if param.grad is not None:\n",
        "            #         param.grad.data.clamp_(-clip_value, clip_value)\n",
        "            optimizer.step()\n",
        "            #---gradient---vvv\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad == None:\n",
        "                    epoch_gradient[name + 'zero'] = 1\n",
        "                elif name not in epoch_gradient:\n",
        "                    epoch_gradient[name] = param.grad.clone()\n",
        "                else:\n",
        "                    epoch_gradient[name] += param.grad\n",
        "            #---gradient---^^^\n",
        "        with torch.inference_mode():\n",
        "            for  image in val_loader:\n",
        "\n",
        "                input_img = image['image'].to(torch.float32).to(device)\n",
        "                target_depth = image['depth'].to(torch.float32).to(device)\n",
        "                pred_depth = model(input_img)\n",
        "\n",
        "                val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n",
        "                mean_epoch_loss_val.append(val_loss.item())\n",
        "\n",
        "        if epoch % save_frequency == 0 or epoch == NO_epochs:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n",
        "                'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n",
        "                'valid_loss' : np.mean(mean_epoch_loss_val),\n",
        "                'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n",
        "                'gradients' : epoch_gradient\n",
        "            }\n",
        "\n",
        "            torch.save(checkpoint, 'weight{}.pth'.format(epoch))\n",
        "            source_path = 'weight{}.pth'.format(epoch)\n",
        "            destination_path = '/content/drive/MyDrive/Colab Notebooks/Unet_Depth_Estimation/Checkpoint2/weight' # main account\n",
        "            # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/NYU_Diffusion_Depth_Estimation/Checkpoint' # free account\n",
        "\n",
        "            shutil.copy(source_path, destination_path)\n",
        "\n",
        "        #---計算時間---vvv\n",
        "        end_time = time.time()\n",
        "        exe_time = end_time - start_time\n",
        "        hours, remainder = divmod(exe_time, 3600)\n",
        "        minutes, seconds = divmod(remainder, 60)\n",
        "        #---計算時間---^^^\n",
        "\n",
        "        #-----以下是存loss的---vvv\n",
        "        checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'valid_loss' : np.mean(mean_epoch_loss_val),\n",
        "        'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n",
        "        'time' : exe_time\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, 'loss{}.pth'.format(epoch))\n",
        "        source_path = 'loss{}.pth'.format(epoch)\n",
        "        destination_path = '/content/drive/MyDrive/Colab Notebooks/Unet_Depth_Estimation/Checkpoint2/loss' # main\n",
        "        # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/NYU_Diffusion_Depth_Estimation/Checkpoint' # free\n",
        "        shutil.copy(source_path, destination_path)\n",
        "        #-----以下是存loss的---^^^\n",
        "\n",
        "        print('---')\n",
        "        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n",
        "        print(\"time = {}:{}:{}\".format(int(hours), int(minutes), int(seconds)))\n",
        "\n"
      ],
      "metadata": {
        "id": "yhvXAjHftr0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not first_epoch:\n",
        "\n",
        "\n",
        "    load_path = '/content/drive/MyDrive/Colab Notebooks/Unet_Depth_Estimation/Checkpoint2/weight/weight{}.pth'.format(start)\n",
        "    checkpoint = torch.load(load_path, map_location = torch.device(device))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    for epoch in range(start + 1, NO_epochs + 1):\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print('lr in the last epoch: {}'.format(current_lr))\n",
        "        start_time = time.time()\n",
        "        mean_epoch_loss = []\n",
        "        mean_epoch_loss_val = []\n",
        "        epoch_gradient = {}\n",
        "        for image in train_loader:\n",
        "            scheduler.step(epoch)\n",
        "            input_img = image['image'].to(torch.float32).to(device)\n",
        "            target_depth = image['depth'].to(torch.float32).to(device)\n",
        "            pred_depth = model(input_img)\n",
        "            # --------------------exit--------\n",
        "            # sys.exit()\n",
        "            # --------------------exit--------\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n",
        "            # params = list(model.parameters())\n",
        "            # weight_tensor = params[490]\n",
        "            # chains = torch.autograd.grad(loss, weight_tensor, retain_graph=True)\n",
        "            # for chain in chains:\n",
        "            #     print(chain)\n",
        "            mean_epoch_loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            # for param in model.parameters():\n",
        "            #     if param.grad is not None:\n",
        "            #         param.grad.data.clamp_(-clip_value, clip_value)\n",
        "            optimizer.step()\n",
        "            #---gradient---vvv\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad == None:\n",
        "                    epoch_gradient[name + 'zero'] = 1\n",
        "                elif name not in epoch_gradient:\n",
        "                    epoch_gradient[name] = param.grad.clone()\n",
        "                else:\n",
        "                    epoch_gradient[name] += param.grad\n",
        "            #---gradient---^^^\n",
        "        with torch.inference_mode():\n",
        "            for  image in val_loader:\n",
        "                input_img = image['image'].to(torch.float32).to(device)\n",
        "                target_depth = image['depth'].to(torch.float32).to(device)\n",
        "                pred_depth = model(input_img)\n",
        "\n",
        "                val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n",
        "                mean_epoch_loss_val.append(val_loss.item())\n",
        "\n",
        "        if epoch % save_frequency == 0 or epoch == NO_epochs:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n",
        "                'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n",
        "                'valid_loss' : np.mean(mean_epoch_loss_val),\n",
        "                'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n",
        "                'gradients' : epoch_gradient\n",
        "            }\n",
        "\n",
        "            torch.save(checkpoint, 'weight{}.pth'.format(epoch))\n",
        "            source_path = 'weight{}.pth'.format(epoch)\n",
        "            destination_path = '/content/drive/MyDrive/Colab Notebooks/Unet_Depth_Estimation/Checkpoint2/weight' # main account\n",
        "            # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/NYU_Diffusion_Depth_Estimation/Checkpoint' # free account\n",
        "\n",
        "            shutil.copy(source_path, destination_path)\n",
        "\n",
        "        #---計算時間---vvv\n",
        "        end_time = time.time()\n",
        "        exe_time = end_time - start_time\n",
        "        hours, remainder = divmod(exe_time, 3600)\n",
        "        minutes, seconds = divmod(remainder, 60)\n",
        "        #---計算時間---^^^\n",
        "\n",
        "        #-----以下是存loss的---vvv\n",
        "        checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'valid_loss' : np.mean(mean_epoch_loss_val),\n",
        "        'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n",
        "        'time' : exe_time\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, 'loss{}.pth'.format(epoch))\n",
        "        source_path = 'loss{}.pth'.format(epoch)\n",
        "        destination_path = '/content/drive/MyDrive/Colab Notebooks/Unet_Depth_Estimation/Checkpoint2/loss' # main\n",
        "        # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/NYU_Diffusion_Depth_Estimation/Checkpoint' # free\n",
        "        shutil.copy(source_path, destination_path)\n",
        "        #-----以下是存loss的---^^^\n",
        "\n",
        "        print('---')\n",
        "        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n",
        "        print(\"time = {}:{}:{}\".format(int(hours), int(minutes), int(seconds)))"
      ],
      "metadata": {
        "id": "ylCIp4Q-tu2q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b6db9d78-3141-420b-afed-1ddb0d35ae65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'first_epoch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-de820db897ce>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Unet_Depth_Estimation/Checkpoint2/weight/weight{}.pth'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'first_epoch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "T7huwuE-zr_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    image = batch['image'].to(device)\n",
        "    depth = batch['depth'].to(device)\n",
        "    break\n",
        "print(depth.shape)"
      ],
      "metadata": {
        "id": "KwZ9M-BOaXiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ans = model(image)"
      ],
      "metadata": {
        "id": "i2kDVNL-azYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(ans.shape)"
      ],
      "metadata": {
        "id": "-fa4DvQDnZE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for idx in range(len(ans)):\n",
        "#     print(ans[idx].shape)\n"
      ],
      "metadata": {
        "id": "D1LnkNqTdVCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss_function(output, target):\n",
        "    di = target - output\n",
        "    n = (output_height * output_width)\n",
        "    di2 = torch.pow(di, 2)\n",
        "    fisrt_term = torch.sum(di2,(1,2,3))/n\n",
        "    second_term = 0.5*torch.pow(torch.sum(di,(1,2,3)), 2)/ (n**2)\n",
        "    # loss = fisrt_term - second_term\n",
        "    loss = fisrt_term\n",
        "    return loss.mean()"
      ],
      "metadata": {
        "id": "QhDpMDYo-1gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "save_frequency = 5\n"
      ],
      "metadata": {
        "id": "UdSEcwP8E1ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def warmup_scheduler(epoch):\n",
        "    if epoch < 10:\n",
        "        return (epoch / 10)\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "fRC8xgH8Mk9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = LambdaLR(optimizer, lr_lambda = warmup_scheduler)"
      ],
      "metadata": {
        "id": "33NmNXLmMm3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "for epoch in range(1, epochs + 1):\n",
        "    training_loss = train_Unet(epoch)\n",
        "    print(\"epoch : {} | training loss : {}\".format(epoch, training_loss))\n",
        "\n",
        "    # if epoch % 1 == 0:\n",
        "    #     validate_Unet(epoch, training_loss)\n",
        "    # if epoch % 1 == 0:\n",
        "    #     model_file = folder_name + \"/model_\" + str(epoch) + \".pth\"\n",
        "    #     torch.save(model.state_dict(), model_file)"
      ],
      "metadata": {
        "id": "jCL5wU9RSApi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 600\n",
        "start = 485\n",
        "weight_path = '/content/drive/MyDrive/Colab Notebooks/Unet_Depth_Estimation/Checkpoint/weight{}.pth'.format(start)\n",
        "checkpoint = torch.load(weight_path, map_location = torch.device(device))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "for epoch in range(start + 1, epochs + 1):\n",
        "    training_loss = train_Unet_cont(epoch)\n",
        "    print(\"epoch : {} | training loss : {}\".format(epoch, training_loss))"
      ],
      "metadata": {
        "id": "uiQMbmDG3EKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "Yvm3hdsDXo-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ir__aJEQTxb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u9RIvDTohK1e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}